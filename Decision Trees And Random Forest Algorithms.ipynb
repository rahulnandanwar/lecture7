{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tree Based Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree based algorithms are considered to be one of the best and mostly used supervised learning methods. Tree based algorithms \n",
    "empower predictive models with high accuracy, stability and ease of interpretation. Unlike linear models, they map non-linear \n",
    "relationships quite well. They are adaptable at solving any kind of problem at hand (classification or regression).\n",
    "\n",
    "Methods like decision trees, random forest, gradient boosting are being popularly used in all kinds of data science problems. \n",
    "Hence, for every analyst (fresher also), it’s important to learn these algorithms and use them for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree is a type of supervised learning algorithm (having a predefined target variable) that is mostly used in \n",
    "classification problems. It works for both categorical and continuous input and output variables. In this technique, we \n",
    "plit the population or sample into two or more homogeneous sets (or sub-populations) based on most significant \n",
    "splitter / differentiator in input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of decision tree is based on the type of target variable we have. It can be of two types:\n",
    "\n",
    "#### 1) Categorical Variable Decision Tree: -->> Decision Tree Classifier \n",
    "\n",
    "Decision Tree which has categorical target variable then it called as categorical variable decision tree. Example:- In above scenario of student problem, where the target variable was “Student will play cricket or not” i.e. YES or NO.\n",
    "\n",
    "#### 2) Continuous Variable Decision Tree: -->> Decision Tree Regressor\n",
    "\n",
    "Decision Tree has continuous target variable then it is called as Continuous Variable Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:- Let’s say we have a problem to predict whether a customer will pay his renewal premium with an insurance company (yes/ no). Here we know that income of customer is a significant variable but insurance company does not have income details for all customers. Now, as we know this is an important variable, then we can build a decision tree to predict customer income based on occupation, product and various other variables. In this case, we are predicting values for continuous variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Terminology related to Tree based Algorithms\n",
    "\n",
    "Let’s look at the basic terminology used with Decision trees:\n",
    "\n",
    "#### 1) Root Node: \n",
    "    It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "\n",
    "#### 2) Splitting: \n",
    "    It is a process of dividing a node into two or more sub-nodes.\n",
    "#### 3) Decision Node: \n",
    "    When a sub-node splits into further sub-nodes, then it is called decision node.\n",
    "#### 4) Leaf/ Terminal Node: \n",
    "    Nodes do not split is called Leaf or Terminal node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Decision_Tree_2.webp\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Pruning: \n",
    "    When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n",
    "#### 6) Branch / Sub-Tree: \n",
    "    A sub section of entire tree is called branch or sub-tree.\n",
    "#### 7) Parent and Child Node: \n",
    "    A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node.\n",
    "    These are the terms commonly used for decision trees. As we know that every algorithm has advantages and disadvantages, \n",
    "    below are the important factors which one should know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the terms commonly used for decision trees. As we know that every algorithm has advantages and disadvantages, below are the important factors which one should know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Decision Tree Works?\n",
    "\n",
    "As the name suggests, this algorithm works by dividing the whole dataset into a tree-like structure based on some rules and conditions and then gives prediction based on those conditions. Let’s understand the approach to decision tree with a basic scenario. Suppose it’s Friday night and you are not able to decide if you should go out or stay at home. Let the decision tree decide it for you.\n",
    "\n",
    "<img src=\"Decision_tree1.PNG\" width=\"450\">\n",
    "\n",
    "Although we may or may not use the decision tree for such decisions, this was a basic example to help you understand how a decision tree makes a decision.\n",
    "So how did it work?\n",
    "*\tIt selects a root node based on a given condition, e.g. our root node was chosen as time >10 pm.\n",
    "*\tThen, the root node was split into child notes based on the given condition. The right child node in the above figure fulfilled the condition, so no more questions were asked.\n",
    "*\tThe left child node didn’t fulfil the condition, so again it was split based on a new condition.\n",
    "*\tThis process continues till all the conditions are met or if you have predefined the depth of your tree, e.g. the depth of our tree is 3, and it reached there when all the conditions were exhausted.\n",
    "\n",
    "Let’s see how the parent nodes and condition is chosen for the splitting to work.\n",
    "\n",
    "\n",
    "#### Decision Tree for Regression\n",
    "\n",
    "When performing regression with a decision tree, we try to divide the given values of X into distinct and non-overlapping regions, e.g. for a set of possible values X1, X2,..., Xp; we will try to divide them into J distinct and non-overlapping regions R1, R2, . . . , RJ.\n",
    "For a given observation falling into the region Rj, the prediction is equal to the mean of the response(y) values for each training observations(x) in the region Rj. \n",
    "The regions R1,R2, . . . , RJ  are selected in a way to reduce the following sum of squares of residuals :\n",
    "\n",
    "\n",
    "<img src=\"formula1.PNG\" width=\"300\">\n",
    "                                                        \n",
    "Where, yrj (second term) is the mean of all the response variables in the region ‘j’.\n",
    "\n",
    "#### Recursive binary splitting(Greedy approach)\n",
    "\n",
    "As mentioned above, we try to divide the X values into j regions, but it is very expensive in terms of computational time to try to fit every set of X values into j regions. Thus, decision tree opts for a top-down greedy approach in which nodes are divided into two regions based on the given condition, i.e. not every node will be split but the ones which satisfy the condition are split into two branches. It is called greedy because it does the best split at a given step at that point of time rather than looking for splitting a step for a better tree in upcoming steps. It decides a threshold value(say s) to divide the observations into different regions(j) such that the RSS for Xj>= s and Xj <s is minimum.\n",
    "\n",
    "<img src=\"formula2.PNG\" width=\"400\">\n",
    "\n",
    "Here for the above equation, j and s are found such that this equation has the minimum value.\n",
    "The regions R1, R2 are selected based on that value of s and j such that the equation above has the minimum value.\n",
    "Similarly, more regions are split out of the regions created above based on some condition with the same logic. This continues until a stopping criterion (predefined) is achieved.\n",
    "Once all the regions are split, the prediction is made based on the mean of observations in that region.\n",
    "\n",
    "The process mentioned above has a high chance of overfitting the training data as it will be very complex.\n",
    "\n",
    "#### Tree Pruning\n",
    "\n",
    "Tree pruning is the method of trimming down a full tree (obtained through the above process) to reduce the complexity and variance in the data. Just as we regularised linear regression, we can also regularise the decision tree model by adding a new term. \n",
    "\n",
    "<img src=\"formula3.PNG\" width=\"300\">\n",
    "                                       \n",
    "Where, T  is the subtree which is a subset of the full tree T0\n",
    "And α is the non-negative tuning parameter which penalises the MSE with an increase in tree length.\n",
    "By using cross-validation, such values of α and T are selected for which our model gives the lowest test error rate.\n",
    "This is how the decision tree regression model works. Let’s now see the working algorithm of doing classification using a decision tree.\n",
    "Greedy Algorithm\n",
    "As per Hands-on machine learning book “greedy algorithm greedily searches for an optimum split at the top level, then repeats the process at each level. It does not check whether or not the split will lead to the lowest possible impurity several levels down. A greedy algorithm often produces a reasonably good solution, but it is not guaranteed to be the optimal solution.”\n",
    "\n",
    "#### Post-pruning\n",
    "\n",
    "Post-pruning, also known as backward pruning, is the process where the decision tree is generated first and then the non-significant branches are removed. Cross-validation set of data is used to check the effect of pruning and tests whether expanding a node will make an improvement or not. If any improvement is there then we continue by expanding that node else if there is reduction in accuracy then the node not be expanded and should be converted in a leaf node.\n",
    "\n",
    "\n",
    "#### Pre-pruning\n",
    "\n",
    "Pre-pruning, also known as forward pruning, stops the non-significant branches from generating. It uses a condition to decide when should it terminate splitting of some of the branches prematurely as the tree is generated.\n",
    "\n",
    "\n",
    "### Classification Trees\n",
    "\n",
    "Regression trees are used for quantitative data. In the case of qualitative data or categorical data, we use classification trees.  In regression trees, we split the nodes based on RSS criteria, but in classification, it is done using classification error rate, Gini impurity and entropy.\n",
    "Let’s understand these terms in detail.\n",
    "\n",
    "#### Entropy\n",
    "Entropy is the measure of randomness in the data. In other words, it gives the impurity present in the dataset.\n",
    "\n",
    "<img src=\"entropy.PNG\" width=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "#### Randomness or Inpurity\n",
    "\n",
    "\n",
    "<img src=\"Shannons_Entropy_Impurity.png\" width=\"300\">\n",
    "                                           \n",
    "When we split our nodes into two regions and put different observations in both the regions, the main goal is to reduce the entropy i.e. reduce the randomness in the region and divide our data cleanly than it was in the previous node. If splitting the node doesn’t lead into entropy reduction, we try to split based on a different condition, or we stop. \n",
    "A region is clean (low entropy) when it contains data with the same labels and random if there is a mixture of labels present (high entropy).\n",
    "Let’s suppose there are ‘m’ observations and we need to classify them into categories 1 and 2.\n",
    "Let’s say that category 1 has ‘n’ observations and category 2 has ‘m-n’ observations.\n",
    "\n",
    "p= n/m  and    q = m-n/m = 1-p\n",
    "\n",
    "then, entropy for the given set is:\n",
    "\n",
    "\n",
    "          E = -p*log2(p) – q*log2(q) \n",
    "          \n",
    "When all the observations belong to category 1, then p = 1 and all observations belong to category 2, then p =0, int both cases E =0, as there is no randomness in the categories.\n",
    "If half of the observations are in category 1 and another half in category 2, then p =1/2 and q =1/2, and the entropy is maximum, E =1.\n",
    "\n",
    "\n",
    "<img src=\"entropy1.PNG\" width=\"300\">   \n",
    "\n",
    "\n",
    "#### Information Gain\n",
    "\n",
    "Information gain calculates the decrease in entropy after splitting a node. It is the difference between entropies before and after the split. The more the information gain, the more entropy is removed. \n",
    "\n",
    "<img src=\"info_gain.PNG\" width=\"300\">\n",
    "\n",
    "                                 \n",
    "Where, T is the parent node before split and X is the split node from T.\n",
    "\n",
    "A tree which is splitted on basis of entropy and information gain value looks like:\n",
    "\n",
    "<img src=\"entropy_tree.PNG\" width=\"900\">\n",
    "\n",
    "\n",
    "#### Ginni Impurity\n",
    "According to wikipedia, ‘Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labelled if it was randomly labelled according to the distribution of labels in the subset.’\n",
    "It is calculated by multiplying the probability that a given observation is classified into the correct class and sum of all the probabilities when that particular observation is classified into the wrong class.\n",
    "Let’s suppose there are k number of classes and an observation belongs to the class ‘i’, then Ginni impurity is given as:\n",
    "\n",
    "<img src=\"Gini_formula.png\" width=\"300\">\n",
    "                                    \n",
    "Ginni impurity value lies between 0 and 1, 0 being no impurity and 1 denoting random distribution.\n",
    "The node for which the Ginni impurity is least is selected as the root node to split.\n",
    "\n",
    "\n",
    "A tree which is splitted on basis of ginni impurity value looks like:\n",
    "\n",
    "<img src=\"tree_example.PNG\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Impurity and Entropy\n",
    "\n",
    "Gini impurity is used by the CART (classification and regression tree) algorithm for classification trees. Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. As shown in the below graph, the highest Gini score is 0.50 while the highest entropy score possible is 1.0.\n",
    "\n",
    "<img src=\"Gini&Entropy.png\" width=\"300\">\n",
    "\n",
    "Gini impurity measures how heterogeneous or mixed some value is over a set. In decision trees, we find criteria that make a set into more homogenous subsets. Information gain is often used to describe this difference that’s being maximized though that term goes with entropy. But it’s really entropy that is the alternative to Gini impurity. When evaluating gini impurity and entropy, it’s important to remember that the higher the score the higher the impurity and therefore the higher the information gain will be when a split occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini Index has values inside the interval [0, 0.5] whereas the interval of the Entropy is [0, 1]. In the following figure, both of them are represented. The gini index has also been represented multiplied by two to see concretely the differences between them, which are not very significant.\n",
    "\n",
    "<img src=\"entropyVsgini.PNG\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Decision Tree:\n",
    "\n",
    "Advantages of Decision Tree\n",
    "\n",
    "1) Clear Visualization: The algorithm is simple to understand, interpret and visualize as the idea is mostly used in our daily lives. Output of a Decision Tree can be easily interpreted by humans.\n",
    "\n",
    "2) Simple and easy to understand: Decision Tree looks like simple if-else statements which are very easy to understand.\n",
    "\n",
    "3) Decision Tree can be used for both classification and regression problems.\n",
    "\n",
    "4) Decision Tree can handle both continuous and categorical variables.\n",
    "\n",
    "5) No feature scaling required: No feature scaling (standardization and normalization) required in case of Decision Tree as it uses rule based approach instead of distance calculation.\n",
    "\n",
    "6) Handles non-linear parameters efficiently: Non linear parameters don't affect the performance of a Decision Tree unlike curve based algorithms. So, if there is high non-linearity between the independent variables, Decision Trees may outperform as compared to other curve based algorithms.\n",
    "\n",
    "7) Decision Tree can automatically handle missing values.\n",
    "\n",
    "8) Decision Tree is usually robust to outliers and can handle them automatically.\n",
    "\n",
    "9) Less Training Period: Training period is less as compared to Random Forest because it generates only one tree unlike forest of trees in the Random Forest.\n",
    "\n",
    "\n",
    "#### Disadvantages of Decision Tree:\n",
    "\n",
    "\n",
    "1)Overfitting: This is the main problem of the Decision Tree. It generally leads to overfitting of the data which ultimately leads to wrong predictions. In order to fit the data (even noisy data), it keeps generating new nodes and ultimately the tree becomes too complex to interpret. In this way, it loses its generalization capabilities. It performs very well on the trained data but starts making a lot of mistakes on the unseen data.\n",
    "\n",
    "2)High variance: As mentioned in point 1, Decision Tree generally leads to the overfitting of data. Due to the overfitting, there are very high chances of high variance in the output which leads to many errors in the final estimation and shows high inaccuracy in the results. In order to achieve zero bias (overfitting), it leads to high variance.\n",
    "\n",
    "3)Unstable: Adding a new data point can lead to re-generation of the overall tree and all nodes need to be recalculated and recreated.\n",
    "\n",
    "4)Not suitable for large datasets: If data size is large, then one single tree may grow complex and lead to overfitting. So in this case, we should use Random Forest instead of a single Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Ensemble Technique ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Techniques\n",
    "\n",
    "We regularly come across many game shows on television and you must have noticed an option of “Audience Poll”. Most of the times a contestant goes with the option which has the highest vote from the audience and most of the times they win. We can generalize this in real life as well where taking opinions from a majority of people is much more preferred than the opinion of a single person.\n",
    "Ensemble technique has a similar underlying idea where we aggregate predictions from a group of predictors, which may be classifiers or regressors, and most of the times the prediction is better than the one obtained using a single predictor. Such algorithms are called Ensemble methods and such predictors are called Ensembles.\n",
    "\n",
    "Let’s suppose we have ‘n’ predictors:\n",
    "\n",
    "Z1, Z2, Z3, ......., Zn with a standard deviation of σ\n",
    "\n",
    "Var(z) = σ^2\n",
    "\n",
    "If we use single predictors Z1, Z2, Z3, ......., Zn the variance associated with each will be σ2 but the expected value will be the average of all the predictors.\n",
    "\n",
    "Let’s consider the average of the predictors:\n",
    "\n",
    "µ = (Z1 + Z2 + Z3+.......+ Zn)/n\n",
    "\n",
    "if we use µ as the predictor then the expected value still remains the same but see the variance now:\n",
    "\n",
    "variance(µ) = σ^2/n\n",
    "\n",
    "So, the expected value remained ‘µ’ but variance decreases when we use average of all the predictors.\n",
    "\n",
    "This is why taking mean is preferred over using single predictors.\n",
    "\n",
    "Ensemble methods take multiple small models and combine their predictions to obtain a more powerful predictive power.\n",
    "\n",
    "There are few very popular Ensemble techniques which we will talk about in detail such as Bagging, Boosting, stacking etc.\n",
    "\n",
    "<img src=\"ensemble.PNG\"> \n",
    "\n",
    "image courtsey: Google\n",
    "\n",
    "<img src=\"1.PNG\">                           \n",
    "\n",
    "\n",
    "\n",
    "### Bagging (Bootstrap Aggregation)\n",
    "\n",
    "In real life scenarios we don’t have multiple different training sets on which we can train our model separately and at the end combine their result. Here, bootstrapping comes into picture. Bootstrapping is a technique of sampling different sets of data from a given training set by using replacement. After bootstrapping the training dataset, we train model on all the different sets and aggregate the result. This technique is known as Bootstrap Aggregation or Bagging.\n",
    "\n",
    "Let’s see definition of bagging:\n",
    "\n",
    "Bagging is the type of ensemble technique in which a single training algorithm is used on different subsets of the training data where the subset sampling is done with replacement (bootstrap). \n",
    "Once the algorithm is trained on all the subsets, then bagging makes the prediction by aggregating all the predictions made by the algorithm on different subsets. In case of regression, bagging prediction is simply the mean of all the predictions and in the case of classifier, bagging prediction is the most frequent prediction (majority vote) among all the predictions.\n",
    "\n",
    "Bagging is also known as parallel model since we run all models parallely and combine there results at the end.\n",
    "\n",
    "<img src=\"2.PNG\">        \n",
    "\n",
    "<img src=\"3.PNG\">  \n",
    "\n",
    "image courtsey: Google\n",
    "\n",
    "* Advantages of a Bagging Model\n",
    "\n",
    "1)\tBagging significantly decreases the variance without increasing bias. \n",
    "\n",
    "2)\tBagging methods work so well because of diversity in the training data since the sampling is done by bootstraping.\n",
    "\n",
    "3)\tAlso, if the training set is very huge, it can save computional time by training model on relatively smaller data set and still can increase the accuracy of the model.\n",
    "\n",
    "4) Works well with small datasets as well.\n",
    "\n",
    "* **Disadvantage of a Bagging Model\n",
    "\n",
    "The main disadvantage of Bagging is that it improves the accuracy of the model on the expense of interpretability i.e. if a single tree was being used as the base model, then it would have a more attarctive and easily interpretable diagram, but with use of bagging this interpretability gets lost.\n",
    "\n",
    "## Pasting\n",
    "\n",
    "Pasting is an ensemble technique similar to bagging with the only difference being that there is no replacement done while sampling the training dataset. This causes less diversity in the sampled datasets and data ends up being correlated. That's why bagging is more preffered than pasting in real scenarios.\n",
    "\n",
    "## Out-of-Bag Evaluation\n",
    "\n",
    "In bagging, when different samples are collected, no sample contains all the data but a fraction of the original dataset.\n",
    "There might be some data which are never sampled at all. The remaining data which are not sampled are called out of bag instances. Since the model never trains over these data, they can be used for evaluating the accuracy of the model by using these data for predicition. We do not need validation set or cross validation and can use out of bag instances for that purpose.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Algorithm\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "Decision trees are one of such models which have low bias but high variance. We have studied that decision trees tend to overfit the data. So bagging technique becomes a very good solution for decreasing the variance in a decision tree.\n",
    "Instead of using a bagging model with underlying model as a decision tree, we can also use Random forest which is more convenient and well optimized for decision trees. The main issue with bagging is that there is not much independence among the sampled datasets i.e. there is correlation. The advantage of random forests over bagging models is that the random forests makes a tweak in the working algorithm of bagging model to decrease the correlation in trees.  The idea is to introduce more randomness while creating trees which will help in reducing correlation.\n",
    "\n",
    "Let’s understand how algorithm works for a random forest model:\n",
    "\n",
    "1)\tJust like in bagging, different samples are collected from the training dataset using bootstraping.\n",
    "\n",
    "2)\tOn each sample we train our tree model and we allow the trees to grow with high depths. \n",
    "\n",
    "    Now, the difference with in random forest is how the trees are formed. In bootstraping we allow all the sample data to be used for splitting the nodes but not   with random forests.  When building a decision tree, each time a split is to happen, a random sample of ‘m’ predictors are chosen from the total ‘p’ predictors. Only those ‘m’ predictors are allowed to be used for the split.\n",
    "\n",
    "    Why is that?\n",
    "\n",
    "    Suppose in those ‘p’ predictors, 1 predictor is very strong. Now each sample this predictor will remain the strongest. So, whenever trees will be built for these sampled data, this predictor will be chosen by all the trees for splitting and thus will result in similar kind of tree formation for each bootstrap model. This introduces correaltion in the dataset and averaging correalted dataset results do not lead low variance. That’s why in random forest the choice for selecting node for split is limited and it introduces randomness in the formation of the trees as well.\n",
    "    Most of the predictors are not allowed to be considered for split.\n",
    "    Generally, value of ‘m’ is taken as m ≈√p , where ‘p’ is the number of predictors in the sample.\n",
    "\n",
    "    When m=p , the random forest model becomes bagging model.   \n",
    "              \n",
    "    *This method is also referred as “Feature Sampling”\n",
    "\n",
    "<img src=\"7.PNG\">\n",
    "\n",
    "\n",
    "    The above graph represents the decrease in test classifcation error as we select different     \n",
    "    values  of ‘m’.\n",
    "\n",
    "3)\tOnce the trees are formed, prediction is made by the random forest by aggregating the predictions of all the model.  For regression model, the mean of all the predictions is the final prediction and for classification mode, the mode of all the predictions is considered the final predictions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages and Disadvantages of Random Forest:\n",
    "\n",
    "1)\tIt can be used for both regression and classification problems.\n",
    "\n",
    "2)\tSince base model is a tree, handling of missing values is easy.\n",
    "\n",
    "3)\tIt gives very accurate result with very low variance.\n",
    "\n",
    "4)\tResults of a random forest are very hard to interpret in comparison with decision trees.\n",
    "\n",
    "5)\tHigh computational time than other respective models.\n",
    "\n",
    "\n",
    "Random Forest should be used where accuracy is up utmost priority and interpretability is not very important. Also, computational time is less expensive than the desired outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are hyper parameters?\n",
    "<img src=\"hypr_params.PNG\" width=\"700\">\n",
    "\n",
    "\n",
    "We can see above the decision tree classifier algorithm takes all those parameters which are also known as hyperparameters.\n",
    "\n",
    "Let's see the most important ones of the parameters(as per sklearn documentation) :\n",
    "#### Parameters\n",
    "  ----------\n",
    " * criterion : string, optional (default=\"gini\")\n",
    "       The function to measure the quality of a split. Supported criteria are\n",
    "       \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "   \n",
    " *  splitter : string, optional (default=\"best\")\n",
    "       The strategy used to choose the split at each node. Supported\n",
    "       strategies are \"best\" to choose the best split and \"random\" to choose\n",
    "       the best random split.\n",
    "   \n",
    " *  max_depth : int or None, optional (default=None)\n",
    "       The maximum depth of the tree. If None, then nodes are expanded until\n",
    "       all leaves are pure or until all leaves contain less than\n",
    "       min_samples_split samples.\n",
    "   \n",
    " *  min_samples_split : int, float, optional (default=2)\n",
    "       The minimum number of samples required to split an internal node:\n",
    "   \n",
    "       - If int, then consider `min_samples_split` as the minimum number.\n",
    "       - If float, then `min_samples_split` is a fraction and\n",
    "         `ceil(min_samples_split * n_samples)` are the minimum\n",
    "         number of samples for each split.\n",
    "   \n",
    "       .. versionchanged:: 0.18\n",
    "          Added float values for fractions.\n",
    "   \n",
    " *  min_samples_leaf : int, float, optional (default=1)\n",
    "       The minimum number of samples required to be at a leaf node.\n",
    "       A split point at any depth will only be considered if it leaves at\n",
    "       least ``min_samples_leaf`` training samples in each of the left and\n",
    "       right branches.  This may have the effect of smoothing the model,\n",
    "       especially in regression.\n",
    "   \n",
    "       - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "       - If float, then `min_samples_leaf` is a fraction and\n",
    "         `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "         number of samples for each node.\n",
    "   \n",
    " *  max_features : int, float, string or None, optional (default=None)\n",
    "       The number of features to consider when looking for the best split:\n",
    "   \n",
    "           - If int, then consider `max_features` features at each split.\n",
    "           - If float, then `max_features` is a fraction and\n",
    "             `int(max_features * n_features)` features are considered at each\n",
    "             split.\n",
    "           - If \"auto\", then `max_features=sqrt(n_features)`.\n",
    "           - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "           - If \"log2\", then `max_features=log2(n_features)`.\n",
    "           - If None, then `max_features=n_features`.\n",
    "   \n",
    "       Note: the search for a split does not stop until at least one\n",
    "       valid partition of the node samples is found, even if it requires to\n",
    "       effectively inspect more than ``max_features`` features.\n",
    "   \n",
    " *  random_state : int, RandomState instance or None, optional (default=None)\n",
    "       If int, random_state is the seed used by the random number generator;\n",
    "       If RandomState instance, random_state is the random number generator;\n",
    "       If None, the random number generator is the RandomState instance used\n",
    "       by `np.random`.\n",
    "   \n",
    " *  max_leaf_nodes : int or None, optional (default=None)\n",
    "       Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
    "       Best nodes are defined as relative reduction in impurity.\n",
    "       If None then unlimited number of leaf nodes.\n",
    "   \n",
    " *  min_impurity_decrease : float, optional (default=0.)\n",
    "       A node will be split if this split induces a decrease of the impurity\n",
    "       greater than or equal to this value.\n",
    "   \n",
    " *  min_impurity_split : float, (default=1e-7)\n",
    "       Threshold for early stopping in tree growth. A node will split\n",
    "       if its impurity is above the threshold, otherwise it is a leaf.\n",
    "       \n",
    " *  class_weight : dict, list of dicts, \"balanced\" or None, default=None\n",
    "       Weights associated with classes in the form ``{class_label: weight}``.\n",
    "       If not given, all classes are supposed to have weight one. For\n",
    "       multi-output problems, a list of dicts can be provided in the same\n",
    "       order as the columns of y.\n",
    "   \n",
    " * presort : bool, optional (default=False)\n",
    "       Whether to presort the data to speed up the finding of best splits in\n",
    "       fitting. For the default settings of a decision tree on large\n",
    "       datasets, setting this to true may slow down the training process.\n",
    "       When using either a smaller dataset or a restricted depth, this may\n",
    "       speed up the training.\n",
    "   \n",
    "\n",
    "When we do hyperparameter tuning, we basically try to find those sets and values of hyperparameters which will give us a model with maximum accuracy.\n",
    "Let's go ahead and try to improve our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation in Python\n",
    "\n",
    "we will use Sklearn module to implement decision tree algorithm. \n",
    "Sklearn uses CART (classification and Regression trees) algorithm and by default it uses Gini impurity as a criteria to split the nodes.\n",
    "\n",
    "There are other algorithms like ID3, C4.5, Chi-square etc. \n",
    "\n",
    "We will see the use of CART in following implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading the Libraries and Dataset\n",
    "    \n",
    "Let’s start by importing the required Python libraries and our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0      Graduate            No   \n",
       "1  LP001003   Male     Yes          1      Graduate            No   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
       "4  LP001008   Male      No          0      Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area Loan_Status  \n",
       "0             1.0         Urban           Y  \n",
       "1             1.0         Rural           N  \n",
       "2             1.0         Urban           Y  \n",
       "3             1.0         Urban           Y  \n",
       "4             1.0         Urban           Y  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing dataset\n",
    "\n",
    "df=pd.read_csv('loan_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and null values imputation\n",
    "# Label Encoding\n",
    "df['Gender']=df['Gender'].map({'Male':1,'Female':0})\n",
    "df['Married']=df['Married'].map({'Yes':1,'No':0})\n",
    "df['Education']=df['Education'].map({'Graduate':1,'Not Graduate':0})\n",
    "df['Dependents'].replace('3+',3,inplace=True)\n",
    "df['Self_Employed']=df['Self_Employed'].map({'Yes':1,'No':0})\n",
    "df['Property_Area']=df['Property_Area'].map({'Semiurban':1,'Urban':2,'Rural':3})\n",
    "df['Loan_Status']=df['Loan_Status'].map({'Y':1,'N':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null Value Imputation\n",
    "rev_null=['Gender','Married','Dependents','Self_Employed','Credit_History','LoanAmount','Loan_Amount_Term']\n",
    "df[rev_null]=df[rev_null].replace({np.nan:df['Gender'].mode(),\n",
    "                                   np.nan:df['Married'].mode(),\n",
    "                                   np.nan:df['Dependents'].mode(),\n",
    "                                   np.nan:df['Self_Employed'].mode(),\n",
    "                                   np.nan:df['Credit_History'].mode(),\n",
    "                                   np.nan:df['LoanAmount'].mean(),\n",
    "                                   np.nan:df['Loan_Amount_Term'].mean()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Creating Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X=df.drop(columns=['Loan_ID','Loan_Status']).values\n",
    "Y=df['Loan_Status'].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train=> (491, 11)\n",
      "Shape of X_test=> (123, 11)\n",
      "Shape of Y_train=> (491,)\n",
      "Shape of Y_test=> (123,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train=>',X_train.shape)\n",
    "print('Shape of X_test=>',X_test.shape)\n",
    "print('Shape of Y_train=>',Y_train.shape)\n",
    "print('Shape of Y_test=>',Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Building and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(criterion = 'entropy', random_state = 42)\n",
    "dt.fit(X_train, Y_train)\n",
    "dt_pred_train = dt.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation F1-Score=> 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluation on Training set\n",
    "dt_pred_train = dt.predict(X_train)\n",
    "print('Training Set Evaluation F1-Score=>',f1_score(Y_train,dt_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Evaluation F1-Score=> 0.7953216374269005\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on Test set\n",
    "dt_pred_test = dt.predict(X_test)\n",
    "print('Testing Set Evaluation F1-Score=>',f1_score(Y_test,dt_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see that the decision tree performs well on in-sample evaluation, but its performance decreases drastically on out-of-sample evaluation. Why do you think that’s the case? Unfortunately, our decision tree model is overfitting on the training data. Will random forest solve this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation F1-Score=> 1.0\n"
     ]
    }
   ],
   "source": [
    "#Building  Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(criterion = 'entropy', random_state = 42)\n",
    "rfc.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluating on Training set\n",
    "rfc_pred_train = rfc.predict(X_train)\n",
    "print('Training Set Evaluation F1-Score=>',f1_score(Y_train,rfc_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Evaluation F1-Score=> 0.8461538461538461\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on Test set\n",
    "rfc_pred_test = rfc.predict(X_test)\n",
    "print('Testing Set Evaluation F1-Score=>',f1_score(Y_test,rfc_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can clearly see that the random forest model performed much better than the decision tree in the out-of-sample evaluation. Let’s discuss the reasons behind this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So Which One Should You Choose – Decision Tree or Random Forest?\n",
    "\n",
    "Random Forest is suitable for situations when we have a large dataset, and interpretability is not a major concern.\n",
    "\n",
    "Decision trees are much easier to interpret and understand. Since a random forest combines multiple decision trees, it becomes more difficult to interpret. Here’s the good news – it’s not impossible to interpret a random forest. Here is an article that talks about interpreting results from a random forest model:\n",
    "\n",
    "Decoding the Black Box: An Important Introduction to Interpretable Machine Learning Models in Python.\n",
    "Also, Random Forest has a higher training time than a single decision tree. You should take this into consideration because as we increase the number of trees in a random forest, the time taken to train each of them also increases. That can often be crucial when you’re working with a tight deadline in a machine learning project.\n",
    "\n",
    "But I will say this – despite instability and dependency on a particular set of features, decision trees are really helpful because they are easier to interpret and faster to train. Anyone with very little knowledge of data science can also use decision trees to make quick data-driven decisions.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier And Regressor\n",
    "\n",
    "Interview Questions:\n",
    "\n",
    "1) Decision Tree\n",
    "\n",
    "2) Entropy, Information Gain, Gini Impurity\n",
    "\n",
    "3) Decision Tree Working For Categorical and Numerical Features\n",
    "\n",
    "4) What are the scenarios where Decision Tree works well\n",
    "\n",
    "5) Decision Tree Low Bias And High Variance- Overfitting\n",
    "\n",
    "6) Hyperparameter Techniques\n",
    "\n",
    "7) Library used for constructing decision tree\n",
    "\n",
    "8) Impact of Outliers Of Decision Tree\n",
    "\n",
    "9) Impact of mising values on Decision Tree\n",
    "\n",
    "10) Does Decision Tree require Feature Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
